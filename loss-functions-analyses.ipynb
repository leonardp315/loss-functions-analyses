{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5bCZRFZ3EJNMRVi50RBw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardp315/loss-functions-analyses/blob/main/loss-functions-analyses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqUG1bB-jftJ"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers sentence-transformers datasets pandas numpy matplotlib tqdm scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Comparative Evaluation of Sentence-Transformer Models with Different Loss Functions\n",
        "for Semantic Similarity and Paraphrase Tasks\n",
        "\n",
        "This script performs a systematic evaluation of different Sentence-Transformer models\n",
        "combined with various loss functions on textual similarity (STS-B) and paraphrase\n",
        "detection (MRPC) datasets.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "\n",
        "# Directory configuration for results\n",
        "RESULTS_DIR = Path(\"results\")\n",
        "FIGURES_DIR = RESULTS_DIR / \"figures\"\n",
        "MODELS_DIR = RESULTS_DIR / \"models\"\n",
        "\n",
        "for directory in [RESULTS_DIR, FIGURES_DIR, MODELS_DIR]:\n",
        "    directory.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Configuration for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Sets seeds for reproducibility across multiple frameworks.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "    # Additional settings for determinism in PyTorch\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    return seed_value\n",
        "\n",
        "SEED = set_seed(42)\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Configuration: Seed={SEED}, Device={DEVICE}\")\n",
        "\n",
        "# Experiment settings\n",
        "SAMPLE_SIZE = None  # Use None for full dataset or a number for sampling\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "SAVE_MODELS = True  # Save trained models\n",
        "\n",
        "# Load and prepare datasets\n",
        "class DatasetLoader:\n",
        "    \"\"\"Manager for loading and preparing textual similarity datasets.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir=None):\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "    def load_dataset(self, name, split='train', sample_size=None, random_state=42):\n",
        "        \"\"\"\n",
        "        Loads and prepares popular textual similarity datasets.\n",
        "\n",
        "        Args:\n",
        "            name: Dataset name ('stsb' or 'mrpc')\n",
        "            split: Dataset partition ('train', 'validation', 'test')\n",
        "            sample_size: Number of examples for sampling (None to use all)\n",
        "            random_state: Seed for reproducible sampling\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with processed data\n",
        "        \"\"\"\n",
        "        if name.lower() == 'stsb':\n",
        "            return self._load_stsb(split, sample_size, random_state)\n",
        "        elif name.lower() == 'mrpc':\n",
        "            return self._load_mrpc(split, sample_size, random_state)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported dataset: {name}. Use 'stsb' or 'mrpc'\")\n",
        "\n",
        "    def _load_stsb(self, split, sample_size, random_state):\n",
        "        \"\"\"Loads the STS-B (Semantic Textual Similarity Benchmark) dataset.\"\"\"\n",
        "        ds = load_dataset('glue', 'stsb', cache_dir=self.cache_dir)[split]\n",
        "        df = pd.DataFrame(ds)\n",
        "\n",
        "        # Label processing\n",
        "        df['label'] = pd.to_numeric(df['label'], errors='coerce')\n",
        "        df = df.dropna(subset=['label'])\n",
        "\n",
        "        # Normalization to [0, 1]\n",
        "        df['label'] = df['label'] / 5.0\n",
        "        df['label'] = df['label'].clip(lower=0.0, upper=1.0)\n",
        "\n",
        "        # Binary label for classification\n",
        "        df['label_bin'] = (df['label'] > 0.5).astype(int)\n",
        "\n",
        "        # Dataset statistics\n",
        "        print(f\"\\n[STS-B - {split}] Statistics:\")\n",
        "        print(f\"- Examples: {len(df)}\")\n",
        "        print(f\"- Similarity range: [{df['label'].min():.2f}, {df['label'].max():.2f}]\")\n",
        "        print(f\"- Binary distribution: {df['label_bin'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Apply sampling if requested\n",
        "        if sample_size is not None:\n",
        "            sample_size = min(sample_size, len(df))\n",
        "            df = df.sample(n=sample_size, random_state=random_state)\n",
        "            print(f\"- Sample used: {sample_size} examples\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _load_mrpc(self, split, sample_size, random_state):\n",
        "        \"\"\"Loads the MRPC (Microsoft Research Paraphrase Corpus) dataset.\"\"\"\n",
        "        ds = load_dataset('glue', 'mrpc', cache_dir=self.cache_dir)[split]\n",
        "        df = pd.DataFrame(ds)\n",
        "\n",
        "        # Ensure labels are integers\n",
        "        df['label'] = df['label'].astype(int)\n",
        "        df['label_bin'] = df['label']\n",
        "\n",
        "        # Dataset statistics\n",
        "        print(f\"\\n[MRPC - {split}] Statistics:\")\n",
        "        print(f\"- Examples: {len(df)}\")\n",
        "        print(f\"- Distribution: {df['label'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Apply sampling if requested\n",
        "        if sample_size is not None:\n",
        "            sample_size = min(sample_size, len(df))\n",
        "            df = df.sample(n=sample_size, random_state=random_state)\n",
        "            print(f\"- Sample used: {sample_size} examples\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def visualize_dataset_distribution(self, df, dataset_name):\n",
        "        \"\"\"Generates visualization of data distribution.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "\n",
        "        if dataset_name.lower() == 'stsb':\n",
        "            sns.histplot(df['label'], bins=20, kde=True)\n",
        "            plt.title('Similarity Distribution in STS-B')\n",
        "            plt.xlabel('Normalized Similarity [0,1]')\n",
        "        else:  # MRPC\n",
        "            counts = df['label'].value_counts().sort_index()\n",
        "            sns.barplot(x=counts.index, y=counts.values)\n",
        "            plt.title('Class Distribution in MRPC')\n",
        "            plt.xlabel('Class (0=Not Paraphrase, 1=Paraphrase)')\n",
        "            plt.xticks([0, 1], ['Not Paraphrase', 'Paraphrase'])\n",
        "\n",
        "        plt.ylabel('Count')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        fig_path = FIGURES_DIR / f\"{dataset_name}_distribution.png\"\n",
        "        plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        return fig_path\n",
        "\n",
        "# Classes for triplet learning\n",
        "class TripletGenerator:\n",
        "    \"\"\"Generator of triplets (anchor, positive, negative) for Triplet Loss.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, fixed_negative=None, hard_negatives=False):\n",
        "        \"\"\"\n",
        "        Initializes the triplet generator.\n",
        "\n",
        "        Args:\n",
        "            dataset: DataFrame with sentence pairs\n",
        "            fixed_negative: Fixed negative sentence (optional)\n",
        "            hard_negatives: If True, selects hard negatives from dataset\n",
        "        \"\"\"\n",
        "        self.dataset = dataset\n",
        "        self.fixed_negative = fixed_negative\n",
        "        self.hard_negatives = hard_negatives\n",
        "\n",
        "    def generate_triplets(self, n_triplets=None):\n",
        "        \"\"\"\n",
        "        Generates sentence triplets for training.\n",
        "\n",
        "        Args:\n",
        "            n_triplets: Number of triplets to generate (default: dataset size)\n",
        "\n",
        "        Returns:\n",
        "            List of triplets (anchor, positive, negative)\n",
        "        \"\"\"\n",
        "        if n_triplets is None:\n",
        "            n_triplets = len(self.dataset)\n",
        "\n",
        "        triplets = []\n",
        "        indices = random.sample(range(len(self.dataset)), k=min(n_triplets, len(self.dataset)))\n",
        "\n",
        "        for i in indices:\n",
        "            anchor = self.dataset.iloc[i]['sentence1']\n",
        "            positive = self.dataset.iloc[i]['sentence2']\n",
        "\n",
        "            if self.fixed_negative:\n",
        "                negative = self.fixed_negative\n",
        "            elif self.hard_negatives:\n",
        "                # Select a different sentence as negative\n",
        "                neg_idx = random.choice([j for j in range(len(self.dataset)) if j != i])\n",
        "                negative = random.choice([self.dataset.iloc[neg_idx]['sentence1'],\n",
        "                                         self.dataset.iloc[neg_idx]['sentence2']])\n",
        "            else:\n",
        "                # Use a random sentence as negative\n",
        "                negative = \"This is a negative sentence for the triplet.\"\n",
        "\n",
        "            triplets.append((anchor, positive, negative))\n",
        "\n",
        "        return triplets\n",
        "\n",
        "class TripletDataset(Dataset):\n",
        "    \"\"\"Triplet dataset compatible with PyTorch DataLoader.\"\"\"\n",
        "\n",
        "    def __init__(self, triplets):\n",
        "        self.triplets = triplets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        a, p, n = self.triplets[idx]\n",
        "        return InputExample(texts=[a, p, n])\n",
        "\n",
        "# Custom loss functions\n",
        "class TripletLoss(torch.nn.Module):\n",
        "    def __init__(self, model, margin=1.0): super().__init__(); self.model = model; self.margin = margin\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        return torch.mean(F.relu(torch.norm(e[0]-e[1], p=2, dim=1) - torch.norm(e[0]-e[2], p=2, dim=1) + self.margin))\n",
        "\n",
        "class OnlineTripletLoss(TripletLoss): pass\n",
        "class BatchHardTripletLoss(TripletLoss): pass\n",
        "class BatchSemiHardTripletLoss(TripletLoss): pass\n",
        "class BatchAllTripletLoss(TripletLoss): pass\n",
        "\n",
        "class MSELoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [self.model(f)['sentence_embedding'] for f in sf]; return F.mse_loss(e[0], e[1])\n",
        "\n",
        "class EuclideanLoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        return torch.mean(torch.norm(e[0] - e[1], p=2, dim=1))\n",
        "\n",
        "class NormalizedEuclideanLoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        distance = torch.norm(e[0] - e[1], p=2, dim=1)\n",
        "        return torch.mean(distance)\n",
        "\n",
        "class AngularMarginLoss(torch.nn.Module):\n",
        "    def __init__(self, model, margin=0.5): super().__init__(); self.model = model; self.margin = margin\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        cosine = torch.sum(e[0] * e[1], dim=1)\n",
        "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        return torch.mean((theta + self.margin * (1.0 - lbl.float())) ** 2)\n",
        "\n",
        "class CircleLoss(torch.nn.Module):\n",
        "    def __init__(self, model, m=0.25, gamma=256): super().__init__(); self.model = model; self.m = m; self.gamma = gamma\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        sim = torch.sum(e[0] * e[1], dim=1)\n",
        "        alpha_p = torch.clamp_min(1 + self.m - sim, min=0)\n",
        "        alpha_n = torch.clamp_min(sim + self.m, min=0)\n",
        "        delta_p = 1 - self.m\n",
        "        delta_n = self.m\n",
        "        logits_p = (-self.gamma) * alpha_p * (sim - delta_p)\n",
        "        logits_n = self.gamma * alpha_n * (sim - delta_n)\n",
        "        loss = torch.log1p(torch.exp(logits_n)) + torch.log1p(torch.exp(logits_p))\n",
        "        return loss.mean()\n",
        "\n",
        "class SphereLoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        return torch.mean(1 - torch.sum(e[0] * e[1], dim=1))\n",
        "\n",
        "class HistogramLoss(torch.nn.Module):\n",
        "    def __init__(self, model, num_bins=10): super().__init__(); self.model = model; self.num_bins = num_bins\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        sim = torch.sum(e[0] * e[1], dim=1)\n",
        "        hist_pos = torch.histc(sim[lbl == 1], bins=self.num_bins, min=-1, max=1)\n",
        "        hist_neg = torch.histc(sim[lbl == 0], bins=self.num_bins, min=-1, max=1)\n",
        "        hist_pos /= (torch.sum(hist_pos) + 1e-10)\n",
        "        hist_neg /= (torch.sum(hist_neg) + 1e-10)\n",
        "        return torch.sum((hist_pos - hist_neg) ** 2)\n",
        "\n",
        "class CentroidLoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        pos_mask = (lbl == 1).unsqueeze(1)\n",
        "        neg_mask = (lbl == 0).unsqueeze(1)\n",
        "        pos_centroid = (e[0] * pos_mask).sum(0) / (pos_mask.sum() + 1e-10)\n",
        "        neg_centroid = (e[0] * neg_mask).sum(0) / (neg_mask.sum() + 1e-10)\n",
        "        return F.mse_loss(pos_centroid, neg_centroid)\n",
        "\n",
        "class HyperSphereLoss(torch.nn.Module):\n",
        "    def __init__(self, model, radius=1.0): super().__init__(); self.model = model; self.radius = radius\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [self.model(f)['sentence_embedding'] for f in sf]\n",
        "        norms = [torch.norm(emb, p=2, dim=1) for emb in e]\n",
        "        return torch.mean((norms[0] - self.radius) ** 2 + (norms[1] - self.radius) ** 2)\n",
        "\n",
        "class ProbabilisticLoss(torch.nn.Module):\n",
        "    def __init__(self, model): super().__init__(); self.model = model\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        sim = torch.sum(e[0] * e[1], dim=1)\n",
        "        prob = torch.sigmoid(sim)\n",
        "        return F.binary_cross_entropy(prob, lbl.float())\n",
        "\n",
        "class LiftedStructuredLoss(torch.nn.Module):\n",
        "    def __init__(self, model, margin=1.0): super().__init__(); self.model = model; self.margin = margin\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        dist_matrix = torch.cdist(e[0], e[1], p=2)\n",
        "        pos_mask = (lbl == 1).float()\n",
        "        neg_mask = (lbl == 0).float()\n",
        "        pos_term = torch.log(torch.exp(dist_matrix * pos_mask).sum() + 1)\n",
        "        neg_term = torch.log(torch.exp(-dist_matrix * neg_mask + self.margin).sum() + 1)\n",
        "        return pos_term + neg_term\n",
        "\n",
        "class GeneralPairLoss(torch.nn.Module):\n",
        "    def __init__(self, model, pos_weight=1.0, neg_weight=1.0): super().__init__(); self.model = model; self.pos_weight = pos_weight; self.neg_weight = neg_weight\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        sim = torch.sum(e[0] * e[1], dim=1)\n",
        "        pos_pairs = sim[lbl == 1]\n",
        "        neg_pairs = sim[lbl == 0]\n",
        "        pos_loss = self.pos_weight * torch.mean((1 - pos_pairs) ** 2)\n",
        "        neg_loss = self.neg_weight * torch.mean(neg_pairs ** 2)\n",
        "        return pos_loss + neg_loss\n",
        "\n",
        "class AngularLoss(torch.nn.Module):\n",
        "    def __init__(self, model, angle_bound=1.0): super().__init__(); self.model = model; self.angle_bound = angle_bound\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        cos_theta = torch.sum(e[0] * e[1], dim=1)\n",
        "        theta = torch.acos(torch.clamp(cos_theta, -1.0 + 1e-7, 1.0 - 1e-7))\n",
        "        target = lbl.float()\n",
        "        return torch.mean(target * theta + (1 - target) * torch.clamp(self.angle_bound - theta, min=0.0))\n",
        "\n",
        "class MarginRankingLoss(torch.nn.Module):\n",
        "    def __init__(self, model, margin=0.5): super().__init__(); self.model = model; self.margin = margin\n",
        "    def forward(self, sf, lbl):\n",
        "        e = [F.normalize(self.model(f)['sentence_embedding'], p=2, dim=1) for f in sf]\n",
        "        sim = torch.sum(e[0] * e[1], dim=1)\n",
        "        target = 2 * lbl.float() - 1\n",
        "        return torch.mean(torch.clamp(self.margin - target * sim, min=0.0))\n",
        "\n",
        "# Dictionary with loss functions\n",
        "loss_functions = {\n",
        "    'MSE': MSELoss,\n",
        "    'Cosine': losses.CosineSimilarityLoss,\n",
        "    'Contrastive': losses.ContrastiveLoss,\n",
        "    'InfoNCE': losses.MultipleNegativesRankingLoss,\n",
        "    'Euclidean': EuclideanLoss,\n",
        "    'NormaEuc': NormalizedEuclideanLoss,\n",
        "    'NPairs': losses.BatchAllTripletLoss,\n",
        "    'MultiSimilarity': losses.MultipleNegativesRankingLoss,\n",
        "    'AngularMargin': AngularMarginLoss,\n",
        "    'Sphere': SphereLoss,\n",
        "    'HyperSphere': HyperSphereLoss,\n",
        "    'Probabilistic': ProbabilisticLoss,\n",
        "    'LiftedStructured': LiftedStructuredLoss,\n",
        "    'GeneralPair': GeneralPairLoss,\n",
        "    'Angular': AngularLoss,\n",
        "    'MarginRanking': MarginRankingLoss,\n",
        "    'Triplet': TripletLoss,\n",
        "    'OnlineTriplet': OnlineTripletLoss,\n",
        "    'BatchHardTriplet': BatchHardTripletLoss,\n",
        "    'BatchSemiHardTriplet': BatchSemiHardTripletLoss,\n",
        "    'BatchAllTriplet': BatchAllTripletLoss\n",
        "}\n",
        "\n",
        "def generate_examples(df, loss_name, fixed_negative=None):\n",
        "    \"\"\"\n",
        "    Generates training examples compatible with different loss functions.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with data\n",
        "        loss_name: Name of loss function to use\n",
        "        fixed_negative: Fixed negative sentence for Triplet Loss\n",
        "\n",
        "    Returns:\n",
        "        Dataset with examples formatted for the specified loss function\n",
        "    \"\"\"\n",
        "    if 'Triplet' in loss_name:\n",
        "        triplets = TripletGenerator(df, fixed_negative, hard_negatives=True).generate_triplets()\n",
        "        return TripletDataset(triplets)\n",
        "    elif loss_name == 'Contrastive':\n",
        "        # For Contrastive Loss, we use binary labels\n",
        "        examples = [InputExample(texts=[r['sentence1'], r['sentence2']], label=float(r['label_bin']))\n",
        "                    for _, r in df.iterrows()]\n",
        "        return examples\n",
        "    else:\n",
        "        # For other loss functions, we use continuous similarity\n",
        "        examples = [InputExample(texts=[r['sentence1'], r['sentence2']], label=float(r['label']))\n",
        "                    for _, r in df.iterrows()]\n",
        "        return examples\n",
        "\n",
        "# Evaluation functions\n",
        "def evaluate_model(model, test_df, dataset_name):\n",
        "    \"\"\"\n",
        "    Evaluates a model on a test dataset.\n",
        "\n",
        "    Args:\n",
        "        model: Trained SentenceTransformer model\n",
        "        test_df: DataFrame with test data\n",
        "        dataset_name: Dataset name ('stsb' or 'mrpc')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    # Prepare data\n",
        "    sent1 = test_df['sentence1'].tolist()\n",
        "    sent2 = test_df['sentence2'].tolist()\n",
        "    labels = test_df['label'].tolist()\n",
        "\n",
        "    # Calculate embeddings and similarities\n",
        "    embeddings = model.encode(sent1 + sent2, batch_size=32, show_progress_bar=False)\n",
        "    embeddings1 = embeddings[:len(sent1)]\n",
        "    embeddings2 = embeddings[len(sent1):]\n",
        "\n",
        "    # Calculate cosine similarities\n",
        "    similarities = []\n",
        "    for e1, e2 in zip(embeddings1, embeddings2):\n",
        "        similarities.append(cosine_similarity([e1], [e2])[0][0])\n",
        "\n",
        "    # Basic metrics\n",
        "    mean_sim = np.mean(similarities)\n",
        "    std_sim = np.std(similarities)\n",
        "    results = {\n",
        "        'mean_similarity': mean_sim,\n",
        "        'std_similarity': std_sim\n",
        "    }\n",
        "\n",
        "    # Dataset-specific metrics\n",
        "    if dataset_name.lower() == 'stsb':\n",
        "        # Correlation for similarity tasks\n",
        "        if len(set(labels)) > 1 and len(set(similarities)) > 1:\n",
        "            results['pearson'] = pearsonr(labels, similarities)[0]\n",
        "            results['spearman'] = spearmanr(labels, similarities)[0]\n",
        "        else:\n",
        "            results['pearson'] = float('nan')\n",
        "            results['spearman'] = float('nan')\n",
        "\n",
        "        # Example for debugging\n",
        "        print(\"\\n[STS-B] Evaluation example:\")\n",
        "        for i in range(min(3, len(labels))):\n",
        "            print(f\"  Label: {labels[i]:.2f} | Similarity: {similarities[i]:.2f}\")\n",
        "\n",
        "    elif dataset_name.lower() == 'mrpc':\n",
        "        # Classification metrics\n",
        "        binary_preds = [1 if s >= 0.5 else 0 for s in similarities]\n",
        "        results['accuracy'] = accuracy_score(labels, binary_preds)\n",
        "        results['f1'] = f1_score(labels, binary_preds)\n",
        "        results['precision'] = precision_score(labels, binary_preds)\n",
        "        results['recall'] = recall_score(labels, binary_preds)\n",
        "\n",
        "        # Example for debugging\n",
        "        print(\"\\n[MRPC] Evaluation example:\")\n",
        "        for i in range(min(3, len(labels))):\n",
        "            print(f\"  Label: {labels[i]} | Predicted: {binary_preds[i]} | Similarity: {similarities[i]:.2f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_results(results_df, metric, dataset_name):\n",
        "    \"\"\"\n",
        "    Generates comparative result plots.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with results\n",
        "        metric: Metric to visualize\n",
        "        dataset_name: Dataset name\n",
        "\n",
        "    Returns:\n",
        "        Path to saved figure file\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Filter data for specific dataset\n",
        "    df = results_df[results_df['Dataset'] == dataset_name].copy()\n",
        "\n",
        "    # Prepare grouped bar plot\n",
        "    pivot_df = df.pivot(index='Model', columns='Loss Function', values=metric)\n",
        "\n",
        "    ax = pivot_df.plot(kind='bar', figsize=(12, 8))\n",
        "\n",
        "    # Graph settings\n",
        "    plt.title(f'{metric} by Model and Loss Function - {dataset_name.upper()}', fontsize=14)\n",
        "    plt.xlabel('Model', fontsize=12)\n",
        "    plt.ylabel(metric, fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(title='Loss Function', fontsize=10)\n",
        "\n",
        "    # Add values on bars\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', fontsize=8)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    filename = f\"{dataset_name}_{metric}_comparison.png\"\n",
        "    filepath = FIGURES_DIR / filename\n",
        "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def plot_training_curve(history, model_name, loss_name, dataset_name):\n",
        "    \"\"\"\n",
        "    Plots training curve.\n",
        "\n",
        "    Args:\n",
        "        history: Training history\n",
        "        model_name: Model name\n",
        "        loss_name: Loss function name\n",
        "        dataset_name: Dataset name\n",
        "\n",
        "    Returns:\n",
        "        Path to saved figure file\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Extract history data\n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Plot losses\n",
        "    plt.plot(epochs, history['train_loss'], 'b-', label='Training Loss')\n",
        "\n",
        "    # Add graph information\n",
        "    plt.title(f'Training Curve: {model_name}\\n{loss_name} on {dataset_name}', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "\n",
        "    # Save figure\n",
        "    model_short = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "    filename = f\"{dataset_name}_{model_short}_{loss_name}_training.png\"\n",
        "    filepath = FIGURES_DIR / filename\n",
        "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "# Main training and evaluation function\n",
        "def train_and_evaluate(model_name, dataset_name, loss_name, train_df, test_df,\n",
        "                      epochs=3, batch_size=16, save_model=False):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a model with a specific loss function.\n",
        "\n",
        "    Args:\n",
        "        model_name: Sentence-Transformer model name\n",
        "        dataset_name: Dataset name ('stsb' or 'mrpc')\n",
        "        loss_name: Loss function name\n",
        "        train_df: DataFrame with training data\n",
        "        test_df: DataFrame with test data\n",
        "        epochs: Number of training epochs\n",
        "        batch_size: Batch size\n",
        "        save_model: If True, saves the trained model\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results and metrics\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize model\n",
        "        model = SentenceTransformer(model_name).to(DEVICE)\n",
        "        model_identifier = model_name.split('/')[-1] if '/' in model_name else model_name\n",
        "\n",
        "        # Configure training\n",
        "        fixed_negative = \"This is an example negative sentence for training triplets.\"\n",
        "        dataset = generate_examples(train_df, loss_name, fixed_negative)\n",
        "        dataloader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
        "        loss_fn = loss_functions[loss_name](model)\n",
        "\n",
        "        # Record training history\n",
        "        history = {'train_loss': []}\n",
        "\n",
        "        class LogCallback:\n",
        "            def __init__(self, history):\n",
        "                self.history = history\n",
        "\n",
        "            def on_epoch_end(self, epoch, loss, *args, **kwargs):\n",
        "                self.history['train_loss'].append(loss)\n",
        "\n",
        "        # Execute training\n",
        "        start_time = time.time()\n",
        "        model.fit(\n",
        "            train_objectives=[(dataloader, loss_fn)],\n",
        "            epochs=epochs,\n",
        "            warmup_steps=int(len(dataloader) * 0.1),\n",
        "            show_progress_bar=True,\n",
        "            output_path=None,\n",
        "            callback=LogCallback(history)\n",
        "        )\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        # Evaluate model\n",
        "        evaluation_results = evaluate_model(model, test_df, dataset_name)\n",
        "\n",
        "        # Save model if requested\n",
        "        model_path = None\n",
        "        if save_model:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            model_path = MODELS_DIR / f\"{dataset_name}_{model_identifier}_{loss_name}_{timestamp}\"\n",
        "            model.save(str(model_path))\n",
        "\n",
        "        # Plot training curve\n",
        "        training_plot = plot_training_curve(history, model_name, loss_name, dataset_name)\n",
        "\n",
        "        # Consolidate results\n",
        "        results = {\n",
        "            'Dataset': dataset_name,\n",
        "            'Model': model_name,\n",
        "            'Loss Function': loss_name,\n",
        "            'Training Time (s)': round(training_time, 2),\n",
        "            'Mean Similarity': round(evaluation_results['mean_similarity'], 4),\n",
        "            'STD Similarity': round(evaluation_results['std_similarity'], 4),\n",
        "            'Epochs': epochs,\n",
        "            'Batch Size': batch_size,\n",
        "            'Training Plot': str(training_plot),\n",
        "            'Model Path': str(model_path) if model_path else None\n",
        "        }\n",
        "\n",
        "        # Add specific metrics\n",
        "        if dataset_name.lower() == 'stsb':\n",
        "            results['Pearson'] = round(evaluation_results['pearson'], 4) if 'pearson' in evaluation_results else None\n",
        "            results['Spearman'] = round(evaluation_results['spearman'], 4) if 'spearman' in evaluation_results else None\n",
        "        elif dataset_name.lower() == 'mrpc':\n",
        "            results['Accuracy'] = round(evaluation_results['accuracy'], 4) if 'accuracy' in evaluation_results else None\n",
        "            results['F1 Score'] = round(evaluation_results['f1'], 4) if 'f1' in evaluation_results else None\n",
        "            results['Precision'] = round(evaluation_results['precision'], 4) if 'precision' in evaluation_results else None\n",
        "            results['Recall'] = round(evaluation_results['recall'], 4) if 'recall' in evaluation_results else None\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in train_and_evaluate({model_name}, {dataset_name}, {loss_name}): {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return {\n",
        "            'Dataset': dataset_name,\n",
        "            'Model': model_name,\n",
        "            'Loss Function': loss_name,\n",
        "            'Error': str(e)\n",
        "        }\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # List of models to evaluate\n",
        "    model_names = [\n",
        "        'sentence-transformers/all-mpnet-base-v2',\n",
        "        'sentence-transformers/bert-base-nli-mean-tokens',\n",
        "        'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
        "    ]\n",
        "\n",
        "    # Datasets to evaluate\n",
        "    datasets = ['stsb', 'mrpc']\n",
        "\n",
        "    # Experimental configurations\n",
        "    experiment_config = {\n",
        "        'seed': SEED,\n",
        "        'device': str(DEVICE),\n",
        "        'epochs': NUM_EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'sample_size': SAMPLE_SIZE,\n",
        "        'save_models': SAVE_MODELS,\n",
        "        'models': model_names,\n",
        "        'datasets': datasets,\n",
        "        'loss_functions': list(loss_functions.keys()),\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "\n",
        "    # Save experiment configuration\n",
        "    with open(RESULTS_DIR / \"experiment_config.json\", 'w') as f:\n",
        "        json.dump(experiment_config, f, indent=2)\n",
        "\n",
        "    # Initialize dataset loader\n",
        "    loader = DatasetLoader()\n",
        "\n",
        "    # Results stored here\n",
        "    all_results = []\n",
        "    dataset_figures = {}\n",
        "\n",
        "    # Main loop\n",
        "    for dataset_name in datasets:\n",
        "        print(f\"\\n\\n{'='*60}\")\n",
        "        print(f\"Dataset: {dataset_name.upper()}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load datasets\n",
        "        train_df = loader.load_dataset(dataset_name, 'train', sample_size=SAMPLE_SIZE)\n",
        "        test_df = loader.load_dataset(dataset_name, 'validation', sample_size=min(408, SAMPLE_SIZE if SAMPLE_SIZE else 1000))\n",
        "\n",
        "        # Distribution visualization\n",
        "        dist_fig = loader.visualize_dataset_distribution(train_df, dataset_name)\n",
        "        dataset_figures[dataset_name] = str(dist_fig)\n",
        "\n",
        "        # Data sample\n",
        "        print(f\"\\nData sample ({dataset_name.upper()}):\")\n",
        "        print(train_df[['sentence1', 'sentence2', 'label']].head(3).to_string())\n",
        "\n",
        "        # Run evaluation for each combination\n",
        "        results_dataset = []\n",
        "\n",
        "        for model_name in model_names:\n",
        "            model_short = model_name.split('/')[-1]\n",
        "            print(f\"\\n{'-'*40}\")\n",
        "            print(f\"Model: {model_short}\")\n",
        "            print(f\"{'-'*40}\")\n",
        "\n",
        "            for loss_name in loss_functions.keys():\n",
        "                print(f\"\\nEvaluating {model_short} with {loss_name} on {dataset_name.upper()}...\")\n",
        "\n",
        "                result = train_and_evaluate(\n",
        "                    model_name=model_name,\n",
        "                    dataset_name=dataset_name,\n",
        "                    loss_name=loss_name,\n",
        "                    train_df=train_df,\n",
        "                    test_df=test_df,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    save_model=SAVE_MODELS\n",
        "                )\n",
        "\n",
        "                results_dataset.append(result)\n",
        "                all_results.append(result)\n",
        "\n",
        "                # Immediate result logging\n",
        "                if 'Error' in result:\n",
        "                    print(f\" Error: {result['Error']}\")\n",
        "                else:\n",
        "                    print(f\" Completed: Mean Sim = {result['Mean Similarity']}\")\n",
        "                    if dataset_name.lower() == 'stsb':\n",
        "                        print(f\"   Pearson = {result['Pearson']}\")\n",
        "                    else:\n",
        "                        print(f\"   Accuracy = {result['Accuracy']}, F1 = {result['F1 Score']}\")\n",
        "\n",
        "        # Save results per dataset\n",
        "        results_df = pd.DataFrame(results_dataset)\n",
        "        results_df.to_csv(RESULTS_DIR / f\"results_{dataset_name}.csv\", index=False)\n",
        "\n",
        "        # Generate visualizations\n",
        "        if dataset_name.lower() == 'stsb':\n",
        "            plot_results(results_df, 'Pearson', dataset_name)\n",
        "        else:\n",
        "            plot_results(results_df, 'F1 Score', dataset_name)\n",
        "            plot_results(results_df, 'Accuracy', dataset_name)\n",
        "\n",
        "    # Consolidate all results\n",
        "    all_results_df = pd.DataFrame(all_results)\n",
        "    all_results_df.to_csv(RESULTS_DIR / \"complete_results.csv\", index=False)\n",
        "\n",
        "    # Generate HTML report\n",
        "    generate_html_report(all_results_df, experiment_config, dataset_figures)\n",
        "\n",
        "    print(\"\\n\\nExperiment completed. Results available in:\", RESULTS_DIR)\n",
        "    return all_results_df\n",
        "\n",
        "def generate_html_report(results_df, config, dataset_figures):\n",
        "\n",
        "    \"\"\"\n",
        "    Generates an HTML report with experiment results.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with all results\n",
        "        config: Experiment configuration\n",
        "        dataset_figures: Dictionary with paths to dataset figures\n",
        "    \"\"\"\n",
        "    report_path = RESULTS_DIR / \"experiment_report.html\"\n",
        "\n",
        "    # Prepare result tables\n",
        "    stsb_df = results_df[results_df['Dataset'] == 'stsb'].copy()\n",
        "    mrpc_df = results_df[results_df['Dataset'] == 'mrpc'].copy()\n",
        "\n",
        "    # Select relevant columns\n",
        "    stsb_cols = ['Model', 'Loss Function', 'Training Time (s)', 'Mean Similarity', 'Pearson', 'Spearman']\n",
        "    mrpc_cols = ['Model', 'Loss Function', 'Training Time (s)', 'Mean Similarity', 'Accuracy', 'F1 Score', 'Precision', 'Recall']\n",
        "\n",
        "    # Clean model names for display\n",
        "    for df in [stsb_df, mrpc_df]:\n",
        "        df['Model'] = df['Model'].apply(lambda x: x.split('/')[-1] if '/' in x else x)\n",
        "\n",
        "    # Generate HTML\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>Sentence-Transformers Evaluation Report</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; color: #333; }}\n",
        "            h1, h2, h3 {{ color: #2c3e50; }}\n",
        "            table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}\n",
        "            th {{ background-color: #f2f2f2; color: #333; font-weight: bold; }}\n",
        "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
        "            tr:hover {{ background-color: #f5f5f5; }}\n",
        "            .container {{ max-width: 1200px; margin: 0 auto; padding: 20px; }}\n",
        "            .section {{ margin-bottom: 30px; }}\n",
        "            .best-result {{ font-weight: bold; color: #27ae60; }}\n",
        "            img {{ max-width: 100%; height: auto; margin: 10px 0; border: 1px solid #ddd; }}\n",
        "            .config {{ background-color: #f8f9fa; padding: 15px; border-radius: 4px; margin-bottom: 20px; }}\n",
        "            footer {{ margin-top: 30px; padding-top: 10px; border-top: 1px solid #eee; color: #7f8c8d; font-size: 0.9em; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"container\">\n",
        "            <header>\n",
        "                <h1>Comparative Evaluation of Sentence-Transformer Models</h1>\n",
        "                <p>Report generated at: {config['timestamp']}</p>\n",
        "            </header>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Experiment Configuration</h2>\n",
        "                <div class=\"config\">\n",
        "                    <p><strong>Device:</strong> {config['device']}</p>\n",
        "                    <p><strong>Seed:</strong> {config['seed']}</p>\n",
        "                    <p><strong>Epochs:</strong> {config['epochs']}</p>\n",
        "                    <p><strong>Batch Size:</strong> {config['batch_size']}</p>\n",
        "                    <p><strong>Sample:</strong> {config['sample_size'] if config['sample_size'] else 'Full Dataset'}</p>\n",
        "                    <p><strong>Models:</strong> {', '.join([m.split('/')[-1] if '/' in m else m for m in config['models']])}</p>\n",
        "                    <p><strong>Loss Functions:</strong> {', '.join(config['loss_functions'])}</p>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Results - STS-B (Semantic Similarity)</h2>\n",
        "                <p>Training data distribution:</p>\n",
        "                <img src=\"{dataset_figures['stsb']}\" alt=\"STS-B Distribution\">\n",
        "\n",
        "                <h3>Performance Metrics</h3>\n",
        "                <table>\n",
        "                    <tr>\n",
        "                        <th>Model</th>\n",
        "                        <th>Loss Function</th>\n",
        "                        <th>Time (s)</th>\n",
        "                        <th>Mean Similarity</th>\n",
        "                        <th>Pearson Correlation</th>\n",
        "                        <th>Spearman Correlation</th>\n",
        "                    </tr>\n",
        "                    {stsb_df[stsb_cols].sort_values('Pearson', ascending=False).to_html(index=False, header=False, classes='results-table')}\n",
        "                </table>\n",
        "\n",
        "                <h3>Results Visualization</h3>\n",
        "                <img src=\"{FIGURES_DIR / 'stsb_Pearson_comparison.png'}\" alt=\"Pearson Comparison STS-B\">\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Results - MRPC (Paraphrase Detection)</h2>\n",
        "                <p>Training data distribution:</p>\n",
        "                <img src=\"{dataset_figures['mrpc']}\" alt=\"MRPC Distribution\">\n",
        "\n",
        "                <h3>Performance Metrics</h3>\n",
        "                <table>\n",
        "                    <tr>\n",
        "                        <th>Model</th>\n",
        "                        <th>Loss Function</th>\n",
        "                        <th>Time (s)</th>\n",
        "                        <th>Mean Similarity</th>\n",
        "                        <th>Accuracy</th>\n",
        "                        <th>F1 Score</th>\n",
        "                        <th>Precision</th>\n",
        "                        <th>Recall</th>\n",
        "                    </tr>\n",
        "                    {mrpc_df[mrpc_cols].sort_values('F1 Score', ascending=False).to_html(index=False, header=False, classes='results-table')}\n",
        "                </table>\n",
        "\n",
        "                <h3>Results Visualization</h3>\n",
        "                <img src=\"{FIGURES_DIR / 'mrpc_F1 Score_comparison.png'}\" alt=\"F1 Comparison MRPC\">\n",
        "                <img src=\"{FIGURES_DIR / 'mrpc_Accuracy_comparison.png'}\" alt=\"Accuracy Comparison MRPC\">\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Training Curve Analysis</h2>\n",
        "                <p>Examples of training curves for the best models:</p>\n",
        "\n",
        "                <h3>STS-B (Best model)</h3>\n",
        "                <img src=\"{stsb_df.sort_values('Pearson', ascending=False).iloc[0]['Training Plot']}\" alt=\"Best Curve STS-B\">\n",
        "\n",
        "                <h3>MRPC (Best model)</h3>\n",
        "                <img src=\"{mrpc_df.sort_values('F1 Score', ascending=False).iloc[0]['Training Plot']}\" alt=\"Best Curve MRPC\">\n",
        "            </div>\n",
        "\n",
        "            <div class=\"section\">\n",
        "                <h2>Conclusions</h2>\n",
        "                <p><strong>Best configuration for STS-B:</strong> {stsb_df.sort_values('Pearson', ascending=False).iloc[0]['Model']} with {stsb_df.sort_values('Pearson', ascending=False).iloc[0]['Loss Function']} (Pearson: {stsb_df.sort_values('Pearson', ascending=False).iloc[0]['Pearson']})</p>\n",
        "                <p><strong>Best configuration for MRPC:</strong> {mrpc_df.sort_values('F1 Score', ascending=False).iloc[0]['Model']} with {mrpc_df.sort_values('F1 Score', ascending=False).iloc[0]['Loss Function']} (F1: {mrpc_df.sort_values('F1 Score', ascending=False).iloc[0]['F1 Score']})</p>\n",
        "\n",
        "                <p>General observations:</p>\n",
        "                <ul>\n",
        "                    <li>Loss functions have significant impact on model performance.</li>\n",
        "                    <li>Models specialized in paraphrase tend to perform better in evaluated tasks.</li>\n",
        "                    <li>Training time varies considerably between models.</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "\n",
        "            <footer>\n",
        "                <p>Report automatically generated by Sentence-Transformers evaluation script.</p>\n",
        "                <p>All models and results are available in directory: {RESULTS_DIR}</p>\n",
        "            </footer>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Save report\n",
        "    with open(report_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"HTML report generated at: {report_path}\")\n",
        "    return report_path\n",
        "\n",
        "# Additional functions for advanced analysis\n",
        "\n",
        "def analyze_similarity_metrics_correlation(results_df, dataset_name):\n",
        "    \"\"\"\n",
        "    Analyzes correlation between mean similarity and performance metrics.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with results\n",
        "        dataset_name: Dataset name to analyze\n",
        "\n",
        "    Returns:\n",
        "        Figure with correlation matrix\n",
        "    \"\"\"\n",
        "    # Filter data for specific dataset\n",
        "    df = results_df[results_df['Dataset'] == dataset_name].copy()\n",
        "\n",
        "    # Columns to analyze\n",
        "    if dataset_name.lower() == 'stsb':\n",
        "        cols = ['Mean Similarity', 'STD Similarity', 'Pearson', 'Spearman', 'Training Time (s)']\n",
        "    else:  # MRPC\n",
        "        cols = ['Mean Similarity', 'STD Similarity', 'Accuracy', 'F1 Score',\n",
        "                'Precision', 'Recall', 'Training Time (s)']\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df[cols].corr()\n",
        "\n",
        "    # Visualize correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "    plt.title(f'Metrics Correlation - {dataset_name.upper()}')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    filepath = FIGURES_DIR / f\"{dataset_name}_metric_correlation.png\"\n",
        "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def analyze_time_vs_performance(results_df):\n",
        "    \"\"\"\n",
        "    Analyzes relationship between training time and performance metrics.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with results\n",
        "\n",
        "    Returns:\n",
        "        Figure with scatter plots\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Split into subplots\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 12))\n",
        "\n",
        "    # Data for STS-B\n",
        "    stsb_df = results_df[results_df['Dataset'] == 'stsb'].copy()\n",
        "    stsb_df['Model'] = stsb_df['Model'].apply(lambda x: x.split('/')[-1] if '/' in x else x)\n",
        "\n",
        "    # Data for MRPC\n",
        "    mrpc_df = results_df[results_df['Dataset'] == 'mrpc'].copy()\n",
        "    mrpc_df['Model'] = mrpc_df['Model'].apply(lambda x: x.split('/')[-1] if '/' in x else x)\n",
        "\n",
        "    # Plot for STS-B\n",
        "    ax = axes[0]\n",
        "    for model in stsb_df['Model'].unique():\n",
        "        model_df = stsb_df[stsb_df['Model'] == model]\n",
        "        ax.scatter(model_df['Training Time (s)'], model_df['Pearson'],\n",
        "                  label=model, alpha=0.7, s=80)\n",
        "\n",
        "        # Add labels for each point\n",
        "        for _, row in model_df.iterrows():\n",
        "            ax.annotate(row['Loss Function'],\n",
        "                       (row['Training Time (s)'], row['Pearson']),\n",
        "                       fontsize=8, alpha=0.8)\n",
        "\n",
        "    ax.set_title('STS-B: Pearson Correlation vs. Training Time')\n",
        "    ax.set_xlabel('Training Time (seconds)')\n",
        "    ax.set_ylabel('Pearson Correlation')\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax.legend()\n",
        "\n",
        "    # Plot for MRPC\n",
        "    ax = axes[1]\n",
        "    for model in mrpc_df['Model'].unique():\n",
        "        model_df = mrpc_df[mrpc_df['Model'] == model]\n",
        "        ax.scatter(model_df['Training Time (s)'], model_df['F1 Score'],\n",
        "                  label=model, alpha=0.7, s=80)\n",
        "\n",
        "        # Add labels for each point\n",
        "        for _, row in model_df.iterrows():\n",
        "            ax.annotate(row['Loss Function'],\n",
        "                       (row['Training Time (s)'], row['F1 Score']),\n",
        "                       fontsize=8, alpha=0.8)\n",
        "\n",
        "    ax.set_title('MRPC: F1 Score vs. Training Time')\n",
        "    ax.set_xlabel('Training Time (seconds)')\n",
        "    ax.set_ylabel('F1 Score')\n",
        "    ax.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure\n",
        "    filepath = FIGURES_DIR / \"time_vs_performance.png\"\n",
        "    plt.savefig(filepath, dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    return filepath\n",
        "\n",
        "def analyze_loss_function_impact(results_df):\n",
        "    \"\"\"\n",
        "    Analyzes impact of different loss functions on performance.\n",
        "\n",
        "    Args:\n",
        "        results_df: DataFrame with results\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with impact statistics\n",
        "    \"\"\"\n",
        "    # Statistics per loss function\n",
        "    impact = []\n",
        "\n",
        "    # Analysis for STS-B\n",
        "    stsb_df = results_df[results_df['Dataset'] == 'stsb'].copy()\n",
        "    stsb_metrics = ['Pearson', 'Spearman']\n",
        "\n",
        "    for loss_fn in stsb_df['Loss Function'].unique():\n",
        "        loss_stats = {\n",
        "            'Dataset': 'STS-B',\n",
        "            'Loss Function': loss_fn,\n",
        "            'Count': len(stsb_df[stsb_df['Loss Function'] == loss_fn])\n",
        "        }\n",
        "\n",
        "        for metric in stsb_metrics:\n",
        "            loss_stats[f'Mean {metric}'] = stsb_df[stsb_df['Loss Function'] == loss_fn][metric].mean()\n",
        "            loss_stats[f'Std {metric}'] = stsb_df[stsb_df['Loss Function'] == loss_fn][metric].std()\n",
        "            loss_stats[f'Max {metric}'] = stsb_df[stsb_df['Loss Function'] == loss_fn][metric].max()\n",
        "            loss_stats[f'Min {metric}'] = stsb_df[stsb_df['Loss Function'] == loss_fn][metric].min()\n",
        "\n",
        "        impact.append(loss_stats)\n",
        "\n",
        "    # Analysis for MRPC\n",
        "    mrpc_df = results_df[results_df['Dataset'] == 'mrpc'].copy()\n",
        "    mrpc_metrics = ['Accuracy', 'F1 Score']\n",
        "\n",
        "    for loss_fn in mrpc_df['Loss Function'].unique():\n",
        "        loss_stats = {\n",
        "            'Dataset': 'MRPC',\n",
        "            'Loss Function': loss_fn,\n",
        "            'Count': len(mrpc_df[mrpc_df['Loss Function'] == loss_fn])\n",
        "        }\n",
        "\n",
        "        for metric in mrpc_metrics:\n",
        "            loss_stats[f'Mean {metric}'] = mrpc_df[mrpc_df['Loss Function'] == loss_fn][metric].mean()\n",
        "            loss_stats[f'Std {metric}'] = mrpc_df[mrpc_df['Loss Function'] == loss_fn][metric].std()\n",
        "            loss_stats[f'Max {metric}'] = mrpc_df[mrpc_df['Loss Function'] == loss_fn][metric].max()\n",
        "            loss_stats[f'Min {metric}'] = mrpc_df[mrpc_df['Loss Function'] == loss_fn][metric].min()\n",
        "\n",
        "        impact.append(loss_stats)\n",
        "\n",
        "    # Create DataFrame with statistics\n",
        "    impact_df = pd.DataFrame(impact)\n",
        "\n",
        "    # Save analysis\n",
        "    impact_df.to_csv(RESULTS_DIR / \"loss_functions_impact.csv\", index=False)\n",
        "\n",
        "    return impact_df\n",
        "\n",
        "def extended_experiment():\n",
        "    \"\"\"\n",
        "    Main function that executes the experiment and additional analyses.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Run main experiment\n",
        "        results_df = main()\n",
        "\n",
        "        # Validate we have results for analyses\n",
        "        if results_df is None or len(results_df) == 0:\n",
        "            print(\"No results for additional analyses.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n\\n\" + \"=\"*60)\n",
        "        print(\"Additional Analyses\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Correlation analysis between metrics\n",
        "        print(\"\\nAnalyzing correlation between metrics...\")\n",
        "        analyze_similarity_metrics_correlation(results_df, 'stsb')\n",
        "        analyze_similarity_metrics_correlation(results_df, 'mrpc')\n",
        "\n",
        "        # Time vs. performance analysis\n",
        "        print(\"\\nAnalyzing time vs. performance relationship...\")\n",
        "        analyze_time_vs_performance(results_df)\n",
        "\n",
        "        # Loss function impact analysis\n",
        "        print(\"\\nAnalyzing loss functions impact...\")\n",
        "        impact_df = analyze_loss_function_impact(results_df)\n",
        "\n",
        "        # Analysis summary\n",
        "        print(\"\\nAnalysis Summary:\")\n",
        "        print(f\"- {len(results_df)} model-loss combinations tested\")\n",
        "\n",
        "        for dataset in ['STS-B', 'MRPC']:\n",
        "            print(f\"\\n{dataset}:\")\n",
        "            dataset_impact = impact_df[impact_df['Dataset'] == dataset]\n",
        "\n",
        "            if dataset == 'STS-B':\n",
        "                best_loss = dataset_impact.sort_values('Mean Pearson', ascending=False).iloc[0]\n",
        "                print(f\"- Best loss function: {best_loss['Loss Function']} (Mean Pearson: {best_loss['Mean Pearson']:.4f})\")\n",
        "            else:\n",
        "                best_loss = dataset_impact.sort_values('Mean F1 Score', ascending=False).iloc[0]\n",
        "                print(f\"- Best loss function: {best_loss['Loss Function']} (Mean F1: {best_loss['Mean F1 Score']:.4f})\")\n",
        "\n",
        "        print(\"\\nAdditional analyses completed and saved in:\", RESULTS_DIR)\n",
        "\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            import shutil\n",
        "\n",
        "            zip_path = shutil.make_archive(\"resultados_experimento\", 'zip', RESULTS_DIR)\n",
        "            files.download(zip_path)\n",
        "        except:\n",
        "            print(\"Download automtico indisponvel (executando fora do Google Colab).\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in additional analyses: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Execute if main script\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*80)\n",
        "    print(\"Evaluation of Sentence-Transformer Models for Semantic Similarity\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Settings:\")\n",
        "    print(f\"- Seed: {SEED}\")\n",
        "    print(f\"- Device: {DEVICE}\")\n",
        "    print(f\"- Epochs: {NUM_EPOCHS}\")\n",
        "    print(f\"- Batch Size: {BATCH_SIZE}\")\n",
        "    print(f\"- Sample size: {SAMPLE_SIZE if SAMPLE_SIZE else 'Full dataset'}\")\n",
        "    print(f\"- Results directory: {RESULTS_DIR}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Run complete experiment with additional analyses\n",
        "    extended_experiment()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "qGoTfbsSsSkN",
        "outputId": "6504fa94-088a-4fd2-edf1-34279a6ea3a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46184a032a64>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}